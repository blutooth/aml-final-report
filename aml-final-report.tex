%% LyX 2.1.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


\makeatother

\usepackage{babel}
\begin{document}
Introduction:


\section{The report should describe the rationale for selecting the paper
and the particular experiments for replication}


\subsubsection*{The idea:}

The key motivation for FlyMC is to achieve speed-ups over typical
MCMC, where we would like to sample some parameter $\theta$ of interest
given datapoints $x_{1:N}$. FlyMC does this by estimating a large
data-set's likelihood using smaller subsets of the data. 

To work, FlyMC requires a non-negative function $B_{n}(\theta)$ that
acts as a lower bound on the likelihood, $B_{n}(\theta)\leq L_{n}(\theta)=P(x_{n}|\theta)$.
It also introduces auxiliary variables, which will act as indicators
for the points for which the further computation (evaluations of $L_{n}(\theta)$)
are required. This gives the motivation for the name, FlyMC, since
these auxiliary variables can be thought of as lighting up when further
likelihood evaluations are desired. Finally, with these terms in place,
an intuitive explanation of FlyMc can be given. We have a lower bound
for the likelihood, $B_{n}(\theta)$, which accounts for a large part
of the probability mass on the distribution from $\theta$. However,
there is still a difference $L_{n}(\theta)$-$B_{n}(\theta)$, which
we are uncertain of and which we would like to compute adjustments
for through sampling. FlyMCn constructs a way of sampling from this
residual probability mass, which is hopefully much smaller than $L_{n}$($\theta)$,
and so may result in large reductions in sampling time. The idea is
neatly summarised in the graph below:

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.4]{\string"Screen Shot 2016-04-29 at 17.14.50\string".png}
\par\end{centering}

\protect\caption{Graphical representation of Sampling in Firefly Monte Carlo (http://www.auai.org/uai2014/proceedings/individuals/302.pdf)}


\end{figure}



\subsubsection*{Rationale for selecting the paper: }

We chose FlyMC because of the ubiquity of MCMC sampling methods which
see use not just in the fields of machine learning and Bayesian statistics,
but also in the fields of computational physics, biology and computational
linguistics. If large speed-ups could be obtained using the methods
expounded in FlyMC, this would be of great benefit to many different
academic communities.

Furthermore, in Bayesian statistics, the recent development of MCMC
methods has been a key step in making it possible to compute large
hierarchical models that require integrations over hundreds or even
thousands of unknown parameters, which is an exciting area of further
research. However, to make MCMC work quickly for data of the order
of that seen in the 'Big Data' era, any speed-up that is possible
would be well received. 

In addition to this, FlyMC is based on a mathematically elegant idea.
The idea is that when performing inference, all that is necessary
is to 


\subsubsection*{The Experiments to be replicated: }



In this practical, we conducted the following three experiments:
\begin{itemize}
\item Logistic regression: For this particular task, we tested we tested
FlyMC's preformance for the task of classifying 7s and 9s on MNIST,
with prior $P(\theta)\sim\mathcal{N}(0,I)$. 
\item Softmax classification: For this task, we tested FlyMC's ability to
fit a softmax classification model on three CIFAR-10 classes (airplane,
automobile and bird) using Langevin dynamics 
\item Robust linear regression: For logistic regression task, we generated
synthetic data and evaluated the performance of FlyMC in comparison
to regular MCMC. 
\end{itemize}
The first two of these three experiments were data-sets used in the
paper. However, for the third data-set we decided it would be intersting
to see if the same results claimed by a paper could be found in a
data-set that was not hand-picked by the authors. 


\section*{Main Body }




\section{Theory}

Given independent and identically distributed observations $X=\{x_{n}\}_{n=1}^{N}$,
one is often interested in the \emph{likelihood}

\[
L(\theta)=P(X|\theta)=\prod_{n=1}^{N}P(x_{n}|\theta)=\prod_{n=1}^{N}L_{n}(\theta)
\]


For example, when performing Metropolis-Hastings MCMC to sample a
posterior distribution $P(\theta|X)\propto P(X|\theta)P(\theta)$one
needs to compute the acceptance probability:

\[
A(\theta'|\theta)=\min\left\{ 1,\frac{q(\theta|\theta')L(\theta')P(\theta')}{q(\theta'|\theta)L(\theta)P(\theta)}\right\} 
\]


This means that each iteration requires at least one ($L(\theta)$
can be cached from the prior iteration) evaluation of the likelihood
over all $N$ points. Unfortunately, in the era of ``Big Data''
$N$ may be on the order of petabytes and computing $L(\theta)$ at
each iteration may be prohibitively expensive.

Firefly Monte Carly (FlyMC) is a method which addressses the computational
problem of evaluating $L(\theta)$ for a large number of data points
$N$ through exact Monte Carlo estimates of the of the full dataset
likelihood using smaller, tractable subsets. As an auxilliary variable
method, it works by augmenting the joint distribution with hidden\emph{
brightness }variables$Z=\{z_{n}\in\left\{ 0,1\right\} \}_{n=1}^{N}$to
form a \emph{complete data likelihood}

\[
P(X,Z|\theta)=\prod_{n=1}^{N}P(x_{n}|\theta)P(z_{n}|x_{n},\theta)
\]


Notice that we can recover the desired likelihoods simply through
marginalization

\[
\sum_{Z}P(X,Z|\theta)=\sum_{z_{1}}\dots\sum_{z_{N}}\prod_{n=1}^{N}P(|x_{n}|\theta)P(z_{n}|x_{n},\theta)=\prod_{n=1}^{N}P(|x_{n}|\theta)\sum_{z_{n}}P(z_{n}|x_{n},\theta)=L(\theta)
\]


In FlyMC, the brightness variables are chosen to have Bernoulli distribution

\begin{equation}
P(z_{n}|x_{n},\theta)=\left(\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}\right)^{z_{n}}\left(\frac{B_{n}(\theta)}{L_{n}(\theta)}\right)^{1-z_{n}}\label{eq:brightness}
\end{equation}


where$B_{n}(\theta)$ is a lower bound on the likelihood satisfying

\begin{align}
 & 0\leq B_{n}(\theta)\leq L_{n}(\theta)\label{eq:bn-bound}\\
 & \prod_{n\in A}B_{n}(\theta)\text{ can be efficiently computed for any data subset }A\subset X\label{eq:bn-efficient}
\end{align}

\begin{enumerate}
\item $0\leq B_{n}(\theta)\leq L_{n}(\theta)$
\item $\prod_{n=1}^{N}B_{n}(\theta)$ can be efficiently computed
\end{enumerate}
\ref{eq:bn-bound} is required for \ref{eq:brightness} to be a valid
probability distribution. To see why \ref{eq:bn-efficient} is necessary,
consider computing the complete data likelihood

\begin{align}
P(X,Z|\theta) & =\prod_{n=1}^{N}P(x_{n}|\theta)P(z_{n}|x_{n},\theta)\nonumber \\
 & =\prod_{n=1}^{N}L_{n}(\theta)\left(\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}\right)^{z_{n}}\left(\frac{B_{n}(\theta)}{L_{n}(\theta)}\right)^{1-z_{n}}\nonumber \\
 & =\prod_{x_{n}\in X_{bright}}\left(L_{n}(\theta)-B_{n}(\theta)\right)\prod_{x_{n}\in X_{dim}}\left(B_{n}(\theta)\right)\label{eq:bright-dim-decomposition}\\
 & =\prod_{n=1}^{N}B_{n}(\theta)\prod_{x_{n}\in X_{bright}}\frac{L_{n}(\theta)-B_{n}(\theta)}{B_{n}(\theta)}
\end{align}


where the bright points $X_{bright}=\{x_{n}:z_{n}=1\}$ and dim points
$X_{dim}=\{x_{n}:z_{n}=0\}$give rise to the ``Firefly'' in the
paper title. Notice that as a consequence of our choice for\ref{eq:brightness},\ref{eq:bright-dim-decomposition}
decomposes into two products where $\frac{L_{n}(\theta)-B_{n}(\theta)}{B_{n}(\theta)}$
only needs to be evaluated for $x_{n}\in X_{bright}$. By assuming
\ref{eq:bn-efficient}, the other term $\prod_{n=1}^{N}B_{n}(\theta)$
can be efficiently computed so the computational complexity is determined
by $X_{bright}$.


\subsection{Performance comparison}

Instead of evaluating $L_{n}(\theta)$ over all $N$ points, FlyMC
only evaluates the likelihood over a subset of points $X_{bright}$.
This provides a speedup of $\frac{N}{\#X_{bright}}$per iteration.
However, since the brightness variables $z_{n}$are themselves, to
understand the performance improvements we consider the expected number
of bright points $E[\#X_{bright}]$.

Note that for any point $x_{n}$,\ref{eq:brightness} implies the
probability $x_{n}$is bright is given by $\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}$.
Hence, the number of bright points is given by

\begin{equation}
E[\#X_{bright}]=\sum_{n=1}^{N}E[z_{n}]=\sum_{n=1}^{N}\int P(\theta|X)\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}d\theta\label{eq:num-bright}
\end{equation}


This says that the number of bright points (and hence the speedup
achieved by FlyMC) is related to the gap between the true likelihood
$L_{n}(\theta)$ and the lower bound $B_{n}(\theta)$, weighted by
the posterior $P(\theta|X)$.

This result makes intuitive sense. Consider the extreme case where
for all $n$: $B_{n}(\theta)=0$. Then $P(z_{n}=1|x_{n},\theta)=1$
so every data point is bright (i.e. $X=X_{bright}$) and \ref{eq:bright-dim-decomposition}
reduces to evaluating the full dataset likelihood $\prod_{n=1}^{N}L_{n}(\theta)$
as usual.

In the other extreme, if $B_{n}(\theta)=L_{n}(\theta)$ so all data
points are dim. At first it appears that FlyMC counterintuitively
offers a $\frac{N}{\#X_{bright}}=\frac{N}{0}=\infty$speedup. However,
considering \ref{eq:bright-dim-decomposition} reveals that what is
computed is $\prod_{n=1}^{N}B_{n}(\theta)=\prod_{n=1}^{N}L_{n}(\theta)$,
which by assumption \ref{eq:bn-efficient}can be computed efficiently.
But then computing $L(\theta)=\prod_{n=1}^{N}L_{n}(\theta)$ is efficient
and hence is no longer $O(N)$, contradicting our earlier assumptions.


\subsection{Discussion on the lower bound $B_{n}(\theta)$}

The integral role played by the lower bound $B_{n}(\theta)$ makes
it an important topic of investigation. In this section, we investigate
how reasonable and justified are the assumptions required for developing
FlyMC's theory.

One of our assumptions (\ref{eq:bn-efficient}) was that computation
of $\prod_{n=1}^{N}B_{n}(\theta)$ be efficient new $\theta$. For
example, choosing $B_{n}(\theta)$ to be exponential family will make
computing $\prod_{n=1}^{N}B_{n}(\theta)$ for each new $\theta$ be
$O(1)$ because

\begin{align*}
B_{n}(\theta) & \propto\exp(\langle\eta_{n},T(\theta)\rangle)\\
\prod_{n=1}^{N} & B_{n}(\theta)\propto\exp(\langle\sum_{n=1}^{N}\eta_{n},T(\theta)\rangle)
\end{align*}


and $\sum_{n=1}^{N}\eta_{n}$ can be evaluated once and cached. We
believe that this assumption is quite restrictive and have had some
difficulty coming up with functional forms which are not exponential
family that satisfy this assumption.

The other assumption (\ref{eq:bn-bound}) required $B_{n}(\theta)$
be a non-negative lower bound for $L_{n}(\theta)$. This is trivially
satisfied by setting $B_{n}(\theta)=0$, but to be practical we should
try to find a lower bound with the smallest gap possible. Several
forms of bounds exist for commonly used likelihoods; the original
authors consider the Jaakola and Jordan bound on the log logistic
function. However, this requirement is quite restrictive and unrealistic
for more complex use cases. For example, if $L_{n}(\theta)$ is given
by some complicated Bayes net or neural network, then a valid $B_{n}(\theta)$
for use with FlyMC is not immediately obvious.


\subsubsection{MAP-tuning the lower bound}

In certain cases (e.g. Jaakola and Jordan bound on log logistic),
$B_{n}$ is part of a parametric family of lower bounds \textbf{$\left\{ B_{n}(\cdot,\xi)\right\} _{\xi}$}where
$\xi$ is a free parameter. Since the number of bright points determines
the speedup offered by FlyMC, \ref{eq:num-bright} suggests that we
should choose $\xi$ to minimize $E\left[\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}\right]$
for each datapoint $n$ where the expectation is taken over the posterior
distribution $P(\theta|X)$.

The authors of FlyMC propose a \emph{MAP-tuned FlyMC} variant where
the posterior expectation $E\left[\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}\right]$
is replaced with the MAP point estimate $\frac{L_{n}(\theta_{MAP})-B_{n}(\theta_{MAP})}{L_{n}(\theta_{MAP})}$
and$\xi=\text{argmin}_{\xi}L_{n}(\theta_{MAP})-B_{n}(\theta_{MAP})$.$\theta_{MAP}=\text{\text{argmax}}_{\theta}P(\theta|X)$
can be found using stochastic gradient descent to handle large data-set
concerns.

Our opinions about this method are mixed. While MAP tuning should
provide a tighter lower bound than no tuning, approximating the posterior
with a delta distribution centered at $\theta_{MAP}$seems suspicious.
Other methods which could be considered here include first generating
some posterior samples using an untuned chain or MCMC on a subset
of the data, then tuning $\xi$over the posterior samples.


\section{Implementation}

FlyMC augments traditional MCMC by introducing brightness variables
$Z$ which are Gibbs sampled along with the original chain's state
$\theta$. At a high level, the algorithm consists of the following
steps:
\begin{enumerate}
\item Initialize $\theta^{(0)}$
\item For $t=1,\cdots,N_{iter}$

\begin{enumerate}
\item For $n=1,\cdots,N$: sample $z_{n}^{(t)}\sim\text{Bernoulli}\left(\frac{L_{n}(\theta^{(t)})-B_{n}(\theta^{(t)})}{L_{n}(\theta^{(t)})}\right)$
with $\theta=\theta^{(t)}$fixed
\end{enumerate}
\item Propose $\theta^{(t+1)}\sim q(\theta^{(t+1)}|\theta^{(t)})$
\item Compute acceptance probability $A(\theta^{(t+1)}|\theta^{(t)})=\min\left\{ 1,\frac{q(\theta^{(t)}|\theta^{(t+1)})P(X,Z|\theta^{(t+1)})P(\theta^{(t+1)})}{q(\theta^{(t+1)}|\theta^{(t)})P(X,Z|\theta^{(t)})P(\theta^{(t)})}\right\} $
\item Repeat from step 2 until chain has mixed, yielding a single sample
$z_{n}^{(\infty)}$
\end{enumerate}
In step 4, the complete data likelihood is computed using \ref{eq:bright-dim-decomposition}
and hence only requires evaluating the likelihood over $X_{bright}$.

One possible criticism of the method described above is that step
2(a) requires sampling $z_{n}$from $N$ Bernoulli distributions each
with probability $\frac{L_{n}(\theta)-B_{n}(\theta)}{L_{n}(\theta)}$
and hence direct sampling will again require $N$ evaluations of $L_{n}(\theta)$.
Fortunately, this can be avoided through \emph{implicit sampling},
where a second MH-MCMC is used to sample the $z_{i}$. The motivation
is that ideally many $z_{i}=0$ so it makes sense to only consider
proposals where each dim point has a small $q_{d\to b}\in[0,1]$ probability
of becoming bright. Then, evaluating the acceptance probability only
requires computing $L_{n}(\theta)$ at the $z_{n}$which have been
proposed to change state.


\subsection{Provided reference software}

Reference code is provided by the authors%
\footnote{https://github.com/HIPS/firefly-monte-carlo%
}, which provides implementations FlyMC for logistic regression using
MH-MCMC, softmax using Langevin dynamics, and sparse linear regression
using slice sampling. The reference implementation includes an implicit
sampling scheme for sampling brightness variables $z_{i}$.


\subsection{Our contributions to the project}

However, the only full example provided is a logistic regression example
on toy data and is insufficient for reproducing the results reported
in the paper. We forked%
\footnote{https://github.com/feynmanliang/firefly-monte-carlo%
} the authors' code and added our own modifications, most importantly:
\begin{enumerate}
\item Support for MAP-tuning of the lower bounds, using scipy.optimize
for MAP estimation
\item Implementation of regular full-dataset MCMC, which is achieved by
fixing $\forall i:z_{i}=1$
\item Instrumentation for counting log-likelihood evaluations and outputting
traces of chain state and log likelihood per iteration
\item Integration, parameter tuning, and reproduction of MNIST Logistic
Regression, CIFAR-10 Softmax Classification, and Robust Linear Regression
experimental results
\end{enumerate}
We also fixed a significant bug in some caching code provided with
the reference implementation and submitted a pull-request upstream%
\footnote{https://github.com/HIPS/firefly-monte-carlo/pull/1%
}. The problem was that computations of $\left\{ \frac{L_{n}(\theta)-B_{n}(\theta)}{B_{n}(\theta)}:n\in idx\right\} $were
being cached for vectors $\theta$ and $idx$, but that caching was
performed over the entire $idx$ vector rather than elementwise. For
example, if $idx2\subsetneq idx$, then even though $\left\{ \frac{L_{n}(\theta)-B_{n}(\theta)}{B_{n}(\theta)}:n\in idx2\right\} \subset\left\{ \frac{L_{n}(\theta)-B_{n}(\theta)}{B_{n}(\theta)}:n\in idx\right\} $
the reference implementation would still result in a cache miss because
$idx2\neq idx$. This bug causes the number of likelihood evaluations
to be twice the number of proposed bright points $\#X_{bright}$at
each iteration, which means that FlyMC will only offer speedups if
$\#X_{bright}<\frac{N}{2}$.


\section*{Results: }
\begin{enumerate}
\item Autocorrelation Logistic Regression

\begin{enumerate}
\item 
\begin{figure}[H]
\includegraphics[scale=0.35]{Figures/acf_logistic.png}

\protect\caption{Autocorrelation Logistic Regression}


\end{figure}

\end{enumerate}
\item Evaluations per iteration: 

\begin{enumerate}
\item 
\begin{figure}[H]
\includegraphics[scale=0.35]{Figures/evals_per_iter.png}

\protect\caption{Evaluations Per Iteration}
\end{figure}

\end{enumerate}
\item Log Posterior

\begin{enumerate}
\item 
\begin{figure}[H]
\includegraphics[scale=0.35]{Figures/neg_log_posterior.png}

\protect\caption{Evaluations Per Iteration}
\end{figure}

\end{enumerate}
\item Mix IN

\begin{enumerate}
\item 
\begin{figure}[H]
\includegraphics[scale=0.35]{Figures/trace_flymap.png}\includegraphics[scale=0.35]{Figures/compMCMC}

\protect\caption{Evaluations Per Iteration}
\end{figure}

\end{enumerate}
\item For the Cifar data, we omit to plot autocorrelation
\end{enumerate}

\section*{Conclusion }

\textbf{You should end by critically evaluating your work and suggest
what you would have done given more time.}

FlyMC achieves speed-ups over typical MCMC algorithms by estimating
a large data-set's likelihood using smaller subsets. To work, FlyMC
requires a non-negative function $B_{n}(\theta)$ that acts as a lower
bound on the likelihood, \$ B\_n(\textbackslash{}theta) \textbackslash{}leq
L\_n(\textbackslash{}theta) = P(x\_n | \textbackslash{}theta)\$. In
addition, FlyMC:

-Motivations for why we should be interested in FlyMC

We 

Main Body: -Mathematical motivations for FLYMC as well as implementations

Results: Graphs Generated. 

Conclusion:
\end{document}
